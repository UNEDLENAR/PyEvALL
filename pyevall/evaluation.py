# ============================================================================== 
#
# Copyright (c) 2022 - Permission is granted for use and modification of this file for research, non-commercial purposes. 
#
# This work has been financed by the European Union (NextGenerationEU funds) through the 
# ``Plan de Recuperación, Transformación y Resiliencia'', by the Ministry of Economic Affairs and Digital Transformation and by the UNED University. 
#
# The work has been developed within the project "ESPACIO DE OBSERVACIÓN DE INTELIGENCIA ARTIFICIAL (IA) EN ESPAÑOL”, in the framework of the 
# Convenio C039/21-OT between the public entity RED.ES, M.P. and the UNIVERSIDAD NACIONAL DE EDUCACIÓN A DISTANCIA (UNED). 
#
# @author Jorge Carrillo-de-Albornoz <jcalbornoz@lsi.uned.es> 
# 
# Licensed under the European Union Public License (EUPL) v.1.1. (the "License"); 
# you may not use this file except in compliance with the License. 
# 
# You may obtain a copy of the License at 
#
#         https://commission.europa.eu/content/european-union-public-licence_en 
#
# Unless required by applicable law or agreed to in writing, software distributed under the License is distributed 
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the License for 
# the specific language governing permissions and limitations under the License. 
#
# ============================================================================== 
from pyevall.metrics.metricfactory import MetricFactory
from pyevall.reports.reports import PyEvALLReport, PyEvALLDataframeReport,\
    PyEvALLMetaReport, PyEvALLMetaReportDataFrame
from pyevall.utils.utils import PyEvALLUtils
from pyevall.comparators.comparators import PyEvALLFormat
from pyevall.reports.reports import PyEvALLEmbeddedReport
import uuid


class PyEvALLEvaluation(object):
        
    def __init__(self):
        self.logger = None
        self.evaluation_id=None
        
    def load_evaluation_conf(self, **params):
        self.evaluation_id= str(uuid.uuid4())
        PyEvALLUtils.load_configuration(self.evaluation_id, **params)  
        self.logger = PyEvALLUtils.get_logger(__name__, self.evaluation_id)  

    def remove_active_evaluation(self):
        PyEvALLUtils.remove_active_configuration(self.evaluation_id)  
            
    
    def evaluate_lst(self, lst_pred, goldstandard, lst_metrics, **params):
        """
        This function evaluates a set of metrics on a set of predictions and a gold standard.

        Parameters:
            predictions (list): List of predicted values.
            goldstandard (list): List of actual (ground truth) values.
            lst_metrics (list): List of metric names to be evaluated.
            **params: Dictionary of optional parameters that are passed to metric creation and evaluation.

        Returns:
            Report Object: Evaluation report generated by PyEvALL.

        Example of use:
            >>> predictions = [0.1, 0.2, 0.3]
            >>> goldstandard = [0.2, 0.3, 0.4]
            >>> lst_metrics = ['Accuracy', 'Precision']
            >>> params = {'report': embedded}
            >>> evaluate(predictions, goldstandard, lst_metrics, **params)
        """
        #Set evaluation configuration of PyEvALL
        self.load_evaluation_conf(**params)
                
        self.logger.info("Evaluating the following metrics " + str(lst_metrics))  
        # Create a PyEvALLFormat object to handle parsing and processing
        meta_report = None
        #if PyEvALLUtils.PARAM_REPORT in params and params[PyEvALLUtils.PARAM_REPORT]==PyEvALLUtils.PARAM_OPTION_REPORT_DATAFRAME:
        if PyEvALLUtils.get_active_configuration(self.evaluation_id)[PyEvALLUtils.PARAM_REPORT]==PyEvALLUtils.PARAM_OPTION_REPORT_DATAFRAME:        
            meta_report = PyEvALLMetaReportDataFrame(None)
        else:
            meta_report = PyEvALLMetaReport()

        for num , pred in enumerate(lst_pred, start=1):
            report = self.evaluate(pred, goldstandard, lst_metrics, load_config=False, **params)
            meta_report.add_pyevall_report(report, num)

        #remove the active configuration for concurrence execution
        self.remove_active_evaluation()
        return meta_report
     
          
    def evaluate(self, predictions, goldstandard, lst_metrics, load_config=True, **params):
        """
        This function evaluates a set of metrics on a set of predictions and a gold standard.

        Parameters:
            predictions (list): List of predicted values.
            goldstandard (list): List of actual (ground truth) values.
            lst_metrics (list): List of metric names to be evaluated.
            load_config (boolean): indicate if should load a new configuration or not.
            **params: Dictionary of optional parameters that are passed to metric creation and evaluation.

        Returns:
            Report Object: Evaluation report generated by PyEvALL.

        Example of use:
            >>> predictions = [0.1, 0.2, 0.3]
            >>> goldstandard = [0.2, 0.3, 0.4]
            >>> lst_metrics = ['Accuracy', 'Precision']
            >>> params = {'report': embedded}
            >>> evaluate(predictions, goldstandard, lst_metrics, **params)
        """        
        #Set evaluation configuration of PyEvALL
        if load_config:
            self.load_evaluation_conf(**params)
                
        self.logger.info("Evaluating the following metrics " + str(lst_metrics))  
        # Create a PyEvALLFormat object to handle parsing and processing
        self.pyevall_report= PyEvALLReport()
        self.pyevall_report.init_report() 
        parser = PyEvALLFormat(self.pyevall_report, predictions, goldstandard, self.evaluation_id)  
              
        if parser.valid_execution: 
            # Get comparators for each test case if format is valid 
            testcase_comp = parser.get_pyevall_comparators() 
            # Iterate through each metric in the provided list      
            for m in lst_metrics:
                self.logger.debug("Evaluating the following metric " + m)    
                # Create a metric instance using MetricFactory            
                metric = MetricFactory.get_instance_metric(m, self.evaluation_id)     
                
                if not metric==None:
                    # Evaluate the metric using the created instance and test case comparators
                    if len(testcase_comp)>0:   
                        self.evaluate_metric(metric, testcase_comp, **params)       
                    else:
                        self.pyevall_report.insert_error_metric(metric, PyEvALLReport.METRIC_NOT_TEST_CASE_IN_COMMON_ERROR)         
                else:
                    # Handle the case where the metric is unknown
                    self.pyevall_report.insert_error_metric_unknown(m, PyEvALLReport.METRIC_UNKONW_METRIC_ERROR)
                        
        # Generate and return the evaluation report    
        report=self.generate_report()  
        
        #remove the active configuration for concurrence execution
        if load_config:
            self.remove_active_evaluation()        
        return report      
              
 
    def evaluate_metric(self, metric, lst_comparators, **params):
        """
        Evaluates a given metric on a set of comparators and updates the evaluation report.
    
        Args:
            metric (Metric): The metric object to be evaluated.
            lst_comparators (list): A list of comparators for each test case.
    
        Returns:
            None
        """                
        # Initialize the metric section in the evaluation report
        self.pyevall_report.init_metric(metric)
        sum_tc=0
        valid_tc=0
        
        # Iterate through each comparator
        for comp in lst_comparators:
            # Evaluate the metric using the current comparator                       
            metric.evaluate(comp, **params)         
            
            # Check if the metric has any preconditions fired                           
            #if not len(metric.preconditions)==0:
            if self.pyevall_report.is_error_preconditions(metric.preconditions):
                # Add preconditions and skip average result calculation
                self.pyevall_report.insert_preconditions_metric(metric)
                self.pyevall_report.insert_result_aveg_tc_metric(metric, None)
                return
            else:
                # Otherwise, add the result for the current test case
                if not len(metric.preconditions)==0:
                    self.pyevall_report.insert_preconditions_metric(metric)
                self.pyevall_report.insert_result_testcase_metric(metric, comp.get_testcase())
                if not metric.result[PyEvALLReport.AVERAGE_TAG]==None:
                    sum_tc+=float(metric.result[PyEvALLReport.AVERAGE_TAG])
                    valid_tc+=1
                    
            # Clear metric results for the next iteration    
            metric.clear_results()       
        
        # Calculate and insert average result (if any valid test cases)
        aveg_tc=None    
        if not valid_tc==0:
            aveg_tc=sum_tc/len(lst_comparators)
        self.pyevall_report.insert_result_aveg_tc_metric(metric, aveg_tc)  
                    
        
                
    def generate_report(self):
        """
        Generates an evaluation report in either embedded or full format based on the provided parameters.
       
        Returns:
            str: The generated evaluation report in the requested format.
        """
        if PyEvALLUtils.get_active_configuration(self.evaluation_id)[PyEvALLUtils.PARAM_REPORT]==PyEvALLUtils.PARAM_OPTION_REPORT_EMBEDDED:             
            # Generate embedded report           
            embedded = PyEvALLEmbeddedReport(self.pyevall_report )
            embedded.generate_pyevall_embedded_report()
            return embedded

        elif PyEvALLUtils.get_active_configuration(self.evaluation_id)[PyEvALLUtils.PARAM_REPORT]==PyEvALLUtils.PARAM_OPTION_REPORT_DATAFRAME: 
            # Generate dataframe report           
            df_report = PyEvALLDataframeReport(self.pyevall_report)
            df_report.generate_pyevall_df_report()
            return df_report            
        
        elif PyEvALLUtils.get_active_configuration(self.evaluation_id)[PyEvALLUtils.PARAM_REPORT]==PyEvALLUtils.PARAM_OPTION_REPORT_SIMPLE: 
            # Generate default full report
            return self.pyevall_report             


        

    


    

    

    
  


    
    

    
    




        
        
        
       