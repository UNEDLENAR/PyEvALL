# ============================================================================== 
#
# Copyright (c) 2022 - Permission is granted for use and modification of this file for research, non-commercial purposes. 
#
# This work has been financed by the European Union (NextGenerationEU funds) through the 
# ``Plan de Recuperación, Transformación y Resiliencia'', by the Ministry of Economic Affairs and Digital Transformation and by the UNED University. 
#
# The work has been developed within the project "ESPACIO DE OBSERVACIÓN DE INTELIGENCIA ARTIFICIAL (IA) EN ESPAÑOL”, in the framework of the 
# Convenio C039/21-OT between the public entity RED.ES, M.P. and the UNIVERSIDAD NACIONAL DE EDUCACIÓN A DISTANCIA (UNED). 
#
# @author Jorge Carrillo-de-Albornoz <jcalbornoz@lsi.uned.es> 
# 
# Licensed under the European Union Public License (EUPL) v.1.1. (the "License"); 
# you may not use this file except in compliance with the License. 
# 
# You may obtain a copy of the License at 
#
#         https://commission.europa.eu/content/european-union-public-licence_en 
#
# Unless required by applicable law or agreed to in writing, software distributed under the License is distributed 
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the License for 
# the specific language governing permissions and limitations under the License. 
#
# ============================================================================== 
from pyevall.metrics.metricfactory import MetricFactory
from pyevall.reports.reports import PyEvALLReport, PyEvALLDataframeReport,\
    PyEvALLMetaReport, PyEvALLMetaReportDataFrame
from pyevall.utils.utils import PyEvALLUtils
from pyevall.comparators.comparators import PyEvALLFormat
from pyevall.reports.reports import PyEvALLEmbeddedReport
import logging.config

# Setting up the logger
logging.config.fileConfig(PyEvALLUtils.LOG_FILENAME, disable_existing_loggers=False)
logger = logging.getLogger(__name__)



class PyEvALLEvaluation(object):    
        
        
    def init_report(self):
        self.pyevall_report= PyEvALLReport()
        self.pyevall_report.init_report()     


    def evaluate_lst(self, lst_pred, goldstandard, lst_metrics, **params):
        """
        This function evaluates a set of metrics on a set of predictions and a gold standard.

        Parameters:
            predictions (list): List of predicted values.
            goldstandard (list): List of actual (ground truth) values.
            lst_metrics (list): List of metric names to be evaluated.
            **params: Dictionary of optional parameters that are passed to metric creation and evaluation.

        Returns:
            Report Object: Evaluation report generated by PyEvALL.

        Example of use:
            >>> predictions = [0.1, 0.2, 0.3]
            >>> goldstandard = [0.2, 0.3, 0.4]
            >>> lst_metrics = ['Accuracy', 'Precision']
            >>> params = {'report': embedded}
            >>> evaluate(predictions, goldstandard, lst_metrics, **params)
        """
        
        logger.info("Evaluating the following metrics " + str(lst_metrics))  
        # Create a PyEvALLFormat object to handle parsing and processing
        meta_report = None
        if PyEvALLUtils.PARAM_REPORT in params and params[PyEvALLUtils.PARAM_REPORT]==PyEvALLUtils.PARAM_OPTION_REPORT_DATAFRAME:        
            meta_report = PyEvALLMetaReportDataFrame(None)
        else:
            meta_report = PyEvALLMetaReport()

        for num , pred in enumerate(lst_pred, start=1):
            report = self.evaluate(pred, goldstandard, lst_metrics, **params)
            meta_report.add_pyevall_report(report, num)

        return meta_report
     
          
    def evaluate(self, predictions, goldstandard, lst_metrics, **params):
        """
        This function evaluates a set of metrics on a set of predictions and a gold standard.

        Parameters:
            predictions (list): List of predicted values.
            goldstandard (list): List of actual (ground truth) values.
            lst_metrics (list): List of metric names to be evaluated.
            **params: Dictionary of optional parameters that are passed to metric creation and evaluation.

        Returns:
            Report Object: Evaluation report generated by PyEvALL.

        Example of use:
            >>> predictions = [0.1, 0.2, 0.3]
            >>> goldstandard = [0.2, 0.3, 0.4]
            >>> lst_metrics = ['Accuracy', 'Precision']
            >>> params = {'report': embedded}
            >>> evaluate(predictions, goldstandard, lst_metrics, **params)
        """        
        logger.info("Evaluating the following metrics " + str(lst_metrics))  
        # Create a PyEvALLFormat object to handle parsing and processing
        self.init_report()
        parser = PyEvALLFormat(self.pyevall_report, predictions, goldstandard, **params)  
              
        if parser.valid_execution: 
            # Get comparators for each test case if format is valid 
            testcase_comp = parser.get_pyevall_comparators(**params)   
            if len(testcase_comp)>0:   
                # Iterate through each metric in the provided list      
                for m in lst_metrics:
                    logger.debug("Evaluating the following metric " + m)    
                    # Create a metric instance using MetricFactory            
                    metric = MetricFactory.get_instance_metric(m, **params)     
                    
                    if not metric==None:
                        # Evaluate the metric using the created instance and test case comparators
                        self.evaluate_metric(metric, testcase_comp)                
                    else:
                        # Handle the case where the metric is unknown
                        self.pyevall_report.insert_error_metric_unknown(m, PyEvALLReport.METRIC_UNKONW_METRIC_ERROR)
                        
        # Generate and return the evaluation report            
        return self.generate_report(**params)        
              
 
    def evaluate_metric(self, metric, lst_comparators, **params):
        """
        Evaluates a given metric on a set of comparators and updates the evaluation report.
    
        Args:
            metric (Metric): The metric object to be evaluated.
            lst_comparators (list): A list of comparators for each test case.
            **params (dict): Optional parameters passed to the metric evaluation.
    
        Returns:
            None
        """        
        # Initialize the metric section in the evaluation report
        self.pyevall_report.init_metric(metric)
        sum_tc=0
        valid_tc=0
        
        # Iterate through each comparator
        for comp in lst_comparators:
            # Evaluate the metric using the current comparator                       
            metric.evaluate(comp)         
            
            # Check if the metric has any preconditions fired                           
            if not len(metric.preconditions)==0:
                # Add preconditions and skip average result calculation
                self.pyevall_report.insert_preconditions_metric(metric)
                self.pyevall_report.insert_result_aveg_tc_metric(metric, None)
                return
            else:
                # Otherwise, add the result for the current test case
                self.pyevall_report.insert_result_testcase_metric(metric, comp.get_testcase())
                if not metric.result[PyEvALLReport.AVERAGE_TAG]==None:
                    sum_tc+=float(metric.result[PyEvALLReport.AVERAGE_TAG])
                    valid_tc+=1
                    
            # Clear metric results for the next iteration    
            metric.clear_results()       
        
        # Calculate and insert average result (if any valid test cases)
        aveg_tc=None    
        if not valid_tc==0:
            aveg_tc=sum_tc/len(lst_comparators)
        self.pyevall_report.insert_result_aveg_tc_metric(metric, aveg_tc)  
                       
                
    def generate_report(self, **params):
        """
        Generates an evaluation report in either embedded or full format based on the provided parameters.
    
        Args:
            **params (dict, optional): Keyword arguments controlling the report format.
                - `PyEvALLUtils.PARAM_REPORT`: If set to `PyEvALLUtils.PARAM_REPORT_EMBEDDED`, generates an embedded report.
                - Other parameters may be used by specific report formats (not shown here).
    
        Returns:
            str: The generated evaluation report in the requested format.
        """
        if PyEvALLUtils.PARAM_REPORT in params and params[PyEvALLUtils.PARAM_REPORT]==PyEvALLUtils.PARAM_OPTION_REPORT_EMBEDDED:     
            # Generate embedded report           
            embedded = PyEvALLEmbeddedReport(self.pyevall_report )
            embedded.generate_pyevall_embedded_report()
            return embedded

        elif PyEvALLUtils.PARAM_REPORT in params and params[PyEvALLUtils.PARAM_REPORT]==PyEvALLUtils.PARAM_OPTION_REPORT_DATAFRAME:
            # Generate dataframe report           
            df_report = PyEvALLDataframeReport(self.pyevall_report)
            df_report.generate_pyevall_df_report()
            return df_report            
        
        else:
            # Generate default full report
            return self.pyevall_report             


    


    


    

    

    
  


    
    

    
    




        
        
        
       